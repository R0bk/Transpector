{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "94b988a2-6221-4b37-b2af-457ad71acea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e27cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/rkopel001/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
       "  from .autonotebook import tqdm as notebook_tqdm\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bisect import insort\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Literal, Optional, Union, Sequence, TypedDict\n",
    "from fastapi import FastAPI\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from pydantic import BaseModel\n",
    "from transpector.model import get_available_models, get_pretrained_model_config, load_model, HookPoint, Logits, per_token_losses, sliceByMinShape\n",
    "from jaxtyping import Integer, Float\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de70f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HookName = str\n",
    "Cache = dict[HookName, t.Tensor]\n",
    "\n",
    "NamesFilter = Optional[Union[Callable[[str], bool], Sequence[str]]]\n",
    "Hook = tuple[Union[HookName, Callable[..., Any]], Callable[..., Any]]\n",
    "\n",
    "\n",
    "ModelComponentName = str # Id of a model compnent as a string\n",
    "SliceName = str # Id of a slice as a string\n",
    "\n",
    "Slice = list[int] # Slice of a single dinension[from, to]\n",
    "TensorSlice = list[Slice] # Slice for a whole tensor\n",
    "AblationTypes = Literal['zero', 'freeze'] # Types of ablation we support\n",
    "\n",
    "class ModelComponentSlice(TypedDict):\n",
    "    slice: TensorSlice\n",
    "ModelComponent = dict[ModelComponentName, dict[SliceName, ModelComponentSlice]]\n",
    "\n",
    "class AblationSliceComponents(TypedDict):\n",
    "    slice: TensorSlice\n",
    "    ablationType: AblationTypes\n",
    "AblationsType = dict[ModelComponentName, dict[SliceName, AblationSliceComponents]]\n",
    "\n",
    "ablation_priority: dict[AblationTypes, int] = {'freeze': 1, 'zero': 2}\n",
    "\n",
    "class PatchSliceComponents(TypedDict):\n",
    "    slice: TensorSlice\n",
    "    edges: ModelComponent\n",
    "PatchesType = dict[ModelComponentName, dict[SliceName, PatchSliceComponents]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf36f718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using pad_token, but it is not set yet.\n",
       "Loaded pretrained model gpt2 into HookedTransformer\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Modelling:\n",
    "    def __init__(self):\n",
    "        self.logical_clock = 0\n",
    "\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.model = load_model(self.model_name)\n",
    "        self.model_config = get_pretrained_model_config(self.model_name)\n",
    "        self.available_models = get_available_models()\n",
    "        self.last_prompt: list[str] = []\n",
    "\n",
    "        self.live_cache: Cache = {}\n",
    "        self.last_cache: Cache = {}\n",
    "\n",
    "        self.ablations: AblationsType = {}\n",
    "        self.fwd_ablation_hooks: list[Hook] = []\n",
    "        self.bwd_ablation_hooks: list[Hook] = []\n",
    "\n",
    "        self.patches: PatchesType = {}\n",
    "        self.fwd_patch_hooks: list[Hook] = []\n",
    "        self.bwd_patch_hooks: list[Hook] = []\n",
    "\n",
    "    @property\n",
    "    def session_config(self):\n",
    "        return {\n",
    "            \"model_config\": self.model_config,\n",
    "            \"ablations\": self.ablations\n",
    "        }\n",
    "    \n",
    "    def set_model(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.model = load_model(self.model_name)\n",
    "        self.model_config = get_pretrained_model_config(self.model_name)\n",
    "\n",
    "    def run_with_hooks(\n",
    "            self,\n",
    "            prompt: str | list[str],\n",
    "            custom_fwd_hooks: Optional[list[Hook]]=None,\n",
    "            custom_bwd_hooks: Optional[list[Hook]]=None,\n",
    "            names_filter: NamesFilter = None,\n",
    "            device: Optional[str]=None,\n",
    "            remove_batch_dim: bool=False,\n",
    "            incl_bwd: bool=False,\n",
    "            reset_hooks_end: bool=True,\n",
    "            clear_contexts: bool=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Modified version of run_with_hooks from Transformer Lens\n",
    "\n",
    "        Prompts the model with specified forward and backward hooks and brings in all ablation\n",
    "        and patching hooks set on the modelling object\n",
    "\n",
    "        Args:\n",
    "            prompt: Model prompt, can be batched list or single string \n",
    "            custom_fwd_hooks: A list of (name, hook), where name is\n",
    "                either the name of a hook point or a boolean function on hook names, and hook is the\n",
    "                function to add to that hook point. Hooks with names that evaluate to True are added\n",
    "                respectively.\n",
    "            custom_bwd_hooks: Same as fwd_hooks, but for the backward pass.\n",
    "            names_filter: Limits the global cache using filter (this may impact ablation and \n",
    "                patching hooks)\n",
    "            device: Device to run the model on\n",
    "            remove_batch_dim: Strips the batch dimension\n",
    "            incl_bwd: Enable doing a backward pass in addition to foward\n",
    "            reset_hooks_end (bool): If True, all hooks are removed at the end, including those added\n",
    "                during this run. Default is True.\n",
    "            clear_contexts (bool): If True, clears hook contexts whenever hooks are reset. Default is\n",
    "                False.\n",
    "\n",
    "        Note:\n",
    "            If you want to use backward hooks, set `reset_hooks_end` to False, so the backward hooks\n",
    "            remain active. This function only runs a forward pass.\n",
    "        \"\"\"\n",
    "        \n",
    "        _, fwd, bwd = self.model.get_caching_hooks(\n",
    "            names_filter, incl_bwd, device, remove_batch_dim=remove_batch_dim, cache=self.live_cache\n",
    "        )\n",
    "\n",
    "        if custom_fwd_hooks:\n",
    "            fwd.extend(custom_fwd_hooks)\n",
    "\n",
    "        if custom_bwd_hooks:\n",
    "            bwd.extend(custom_bwd_hooks)\n",
    "\n",
    "        # We want to do ablations before patches and patches before caching activations\n",
    "        fwd = [*self.fwd_ablation_hooks, *self.fwd_patch_hooks, *fwd]\n",
    "        bwd = [*self.bwd_ablation_hooks, *self.bwd_patch_hooks, *bwd]\n",
    "\n",
    "        with self.model.hooks(\n",
    "            fwd_hooks=fwd,\n",
    "            bwd_hooks=bwd,\n",
    "            reset_hooks_end=reset_hooks_end,\n",
    "            clear_contexts=clear_contexts,\n",
    "        ):\n",
    "            model_out_logits, model_out_loss = self.model(prompt, return_type='both')\n",
    "            if incl_bwd:\n",
    "                model_out.backward()\n",
    "\n",
    "        self.last_cache = self.live_cache.copy()\n",
    "        self.live_cache = {} # Reset live cache after run\n",
    "\n",
    "        return model_out_logits, model_out_loss, self.last_cache\n",
    "    \n",
    "    def patch_hook(\n",
    "            self,\n",
    "            result: t.Tensor,\n",
    "            hook: HookPoint,\n",
    "            source_component: ModelComponentName,\n",
    "            source_slice: TensorSlice,\n",
    "            target_component: ModelComponentName,\n",
    "            target_slice: TensorSlice,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        For the purpose of Transpector a patch is a hook that takes an activation from a slice of a\n",
    "        model component and applies it to a slice of another model component.\n",
    "\n",
    "        These two components are called the source and the target, source is where the copy happens\n",
    "        and target is where the paste is applied. To do this we use our cache that has saved the source\n",
    "        previously and add a hook on the target to apply from the saved cache.\n",
    "        \"\"\"\n",
    "\n",
    "        source_py_slice = [slice(r0, r1 if r1!=-1 else None) for (r0, r1) in source_slice]\n",
    "        target_py_slice = [slice(r0, r1 if r1!=-1 else None) for (r0, r1) in target_slice]\n",
    "\n",
    "        if source_component in self.live_cache:\n",
    "            result[target_py_slice] = self.live_cache[source_component][source_py_slice]\n",
    "        elif source_component in self.last_cache and \\\n",
    "            result[target_py_slice].shape == self.last_cache[source_component][source_py_slice].shape:\n",
    "            result[target_py_slice] = self.last_cache[source_component][source_py_slice]\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def ablation_hook(\n",
    "        self,\n",
    "        result: t.Tensor,\n",
    "        hook: HookPoint,\n",
    "        slices: TensorSlice,\n",
    "        ablation_type: AblationTypes='zero',\n",
    "    ) -> t.Tensor:\n",
    "        assert hook.name\n",
    "        py_slice = [slice(r0, r1 if r1!=-1 else None) for (r0, r1) in slices]\n",
    "\n",
    "        if ablation_type == 'zero':\n",
    "            print('albating', hook.name)\n",
    "            result[py_slice] = 0.0\n",
    "        elif ablation_type == 'freeze':\n",
    "            \n",
    "            print('freezing', hook.name)\n",
    "            if hook.name not in self.last_cache:\n",
    "                self.last_cache[hook.name] = result.clone().detach()\n",
    "\n",
    "            cache_slice = self.last_cache[hook.name][py_slice]\n",
    "            res_slice = result[py_slice]\n",
    "            frozen = t.zeros_like(res_slice)\n",
    "            frozen[sliceByMinShape(cache_slice, res_slice)] = cache_slice[sliceByMinShape(cache_slice, res_slice)]\n",
    "\n",
    "            result[py_slice] = frozen\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def clean_cache(cls, cache: Cache):\n",
    "        \"\"\"\n",
    "        # Attrs to keep\n",
    "        'hook_embed',\n",
    "        'hook_pos_embed',\n",
    "        'blocks.*.hook_resid_pre',\n",
    "        'blocks.*.attn.hook_q',\n",
    "        'blocks.*.attn.hook_k',\n",
    "        'blocks.*.attn.hook_v',\n",
    "        'blocks.*.attn.hook_pattern',\n",
    "        'blocks.*.attn.hook_z',\n",
    "        'blocks.*.hook_attn_out',\n",
    "        'blocks.*.hook_resid_mid',\n",
    "        'blocks.*.hook_mlp_out',\n",
    "        'blocks.*.hook_resid_post',\n",
    "        'ln_final.hook_scale',\n",
    "        'ln_final.hook_normalized'\n",
    "\n",
    "        # Attrs to remove\n",
    "        'blocks.*.ln1.*'\n",
    "        'blocks.*.ln2.*'\n",
    "        'blocks.*.attn.hook_attn_scores',\n",
    "        'blocks.*.mlp.hook_pre',\n",
    "        'blocks.*.mlp.hook_post',\n",
    "        \"\"\"\n",
    "        cache_copy: dict[ModelComponentName, Float[list[float], \"...\"]] = {}\n",
    "        for key, value in cache.items():\n",
    "            match key.split('.'):\n",
    "                case ['hook_embed'] | ['hook_pos_embed'] | ['ln_final', 'hook_normalized']:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "                case ['blocks', _blockno, ('hook_resid_pre' | 'hook_attn_out' | 'hook_resid_mid' | 'hook_mlp_out' | 'hook_resid_post')]:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "                case ['blocks', _blockno, 'attn', ('hook_q' | 'hook_k' | 'hook_v' | 'hook_z' | 'hook_pattern')]:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "                case _:\n",
    "                    pass\n",
    "\n",
    "        return cache_copy\n",
    "    \n",
    "    def clean_logits(self, logits: Logits):\n",
    "        tokens: Integer[t.Tensor, \"batch seq\"] | Integer[t.Tensor, \"seq\"] = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "        if tokens.dim() == 1:\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "\n",
    "        output_tokens: list[list[int]] = tokens.tolist()\n",
    "        sub_words: list[list[str]] = [self.model.to_str_tokens(t) for t in tokens]\n",
    "        output_logits: list[list[list[float]]] = logits.tolist()\n",
    "\n",
    "        return  output_tokens, sub_words, output_logits\n",
    "\n",
    "    @classmethod\n",
    "    def clean_loss(cls, loss: t.Tensor) -> float:\n",
    "        return loss.item()\n",
    "    \n",
    "    @classmethod\n",
    "    def clean_token_loss(cls, loss: t.Tensor) -> list[list[float]]:\n",
    "        return loss.tolist()\n",
    "\n",
    "\n",
    "ts = Modelling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543b3230-345a-4131-b82d-194057128f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/api/models/getModels\")\n",
    "def get_models(): \n",
    "    return ts.available_models\n",
    "\n",
    "@app.get(\"/api/models/getModelConfig/{model_name}\")\n",
    "def get_model_config(model_name: str): \n",
    "    return {\n",
    "        \"config\": get_pretrained_model_config(model_name),\n",
    "        \"sessionConfig\": ts.session_config \n",
    "    }\n",
    "\n",
    "class ModelItem(BaseModel):\n",
    "    model_name: str\n",
    "\n",
    "@app.put(\"/api/models/setModel\")\n",
    "def set_models(model_name: ModelItem):\n",
    "    print(f'Loading model: {model_name.model_name}')\n",
    "    ts.set_model(model_name.model_name)\n",
    "    print('Loaded model')\n",
    "    return {\"Loaded model\": model_name}\n",
    "\n",
    "class InputStringListItem(BaseModel):\n",
    "    input: list[str]\n",
    "\n",
    "class InputIntListItem(BaseModel):\n",
    "    input: list[int]\n",
    "\n",
    "@app.put(\"/api/tokenize/toStringTokens\")\n",
    "def tokenize_to_string_tokens(inputItem: InputStringListItem):\n",
    "    return {\"stringTokens\": ts.model.to_str_tokens(inputItem.input)}\n",
    "\n",
    "@app.put(\"/api/tokenize/toTokens\")\n",
    "def tokenize_to_tokens(inputItem: InputStringListItem):\n",
    "    ts.last_prompt = inputItem.input\n",
    "    tokens: list[list[int]] = ts.model.to_tokens(inputItem.input).tolist()\n",
    "    print('tokenizing', inputItem.input, 'to', tokens)\n",
    "    return {\"tokens\": tokens}\n",
    "\n",
    "@app.put(\"/api/tokenize/toString\")\n",
    "def tokenize_to_string(inputItem: InputIntListItem):\n",
    "    return {\"string\": ts.model.to_tokens(inputItem.input)}\n",
    "\n",
    "\n",
    "@app.get(\"/api/inference/run\")\n",
    "def inference_run():\n",
    "    prompt = ts.last_prompt\n",
    "    sub_words = ts.model.to_str_tokens(prompt)\n",
    "\n",
    "    logits, loss, cache = ts.run_with_hooks(prompt)\n",
    "    print('logtis are')\n",
    "    print(logits)\n",
    "    out_cache = ts.clean_cache(cache)\n",
    "    out_tokens, out_sub_words, out_logits = ts.clean_logits(logits)\n",
    "    out_final_loss = ts.clean_loss(loss)\n",
    "    out_token_loss = ts.clean_token_loss(per_token_losses(logits, ts.model.to_tokens(prompt)))\n",
    "    return {\n",
    "        \"activationData\": out_cache,\n",
    "        \"inferencePrompt\": prompt,\n",
    "        \"inferenceSubWords\": sub_words,\n",
    "        \"logits\": out_logits,\n",
    "        \"tokens\": out_tokens,\n",
    "        \"subWords\": out_sub_words,\n",
    "        \"finalLoss\": out_final_loss,\n",
    "        \"tokenLoss\": out_token_loss,\n",
    "    }\n",
    "    \n",
    "class InputAblationState(BaseModel):\n",
    "    ablations: AblationsType\n",
    "    clientLogicalClock: int\n",
    "\n",
    "\n",
    "@app.put(\"/api/ablation/sync\")\n",
    "def ablation_sync(input: InputAblationState):\n",
    "\n",
    "    if input.clientLogicalClock >= ts.logical_clock:\n",
    "        ts.logical_clock += 1\n",
    "\n",
    "        temp_hooks_list: list[tuple[int, Hook]] = []\n",
    "\n",
    "        for transformer_component_name, component_slices in input.ablations.items():\n",
    "            print('transformer comp', transformer_component_name)\n",
    "            print('transformer comp', component_slices)\n",
    "\n",
    "            for _target_sub_component, hook_config in component_slices.items():\n",
    "                print('hook config', hook_config)\n",
    "                ablation_hook = partial(\n",
    "                    ts.ablation_hook,\n",
    "                    slices=hook_config['slice'],\n",
    "                    ablation_type=hook_config['ablationType']\n",
    "                )\n",
    "                # Create a tuple with ablationType priority as the first element and the hook as the second\n",
    "                hook_tuple = (ablation_priority[hook_config['ablationType']], (transformer_component_name, ablation_hook))\n",
    "                \n",
    "                # Use insort to insert the tuple in the correct place in the list\n",
    "                insort(temp_hooks_list, hook_tuple, key=lambda x: x[0])\n",
    "\n",
    "                ts.fwd_ablation_hooks.append((transformer_component_name, ablation_hook)) # TODO, can kill this line?\n",
    "\n",
    "        # Reassign fwd_ablation_hooks to contain only the second element of each tuple (preserving the original format)\n",
    "        ts.fwd_ablation_hooks = [hook for _, hook in temp_hooks_list]\n",
    "\n",
    "        ts.ablations = input.ablations\n",
    "\n",
    "    return {\n",
    "        \"server_logical_clock\": ts.logical_clock,\n",
    "        \"ablations\": ts.ablations\n",
    "    }\n",
    "\n",
    "\n",
    "class InputPatchState(BaseModel):\n",
    "    patches: PatchesType\n",
    "    clientLogicalClock: int\n",
    "\n",
    "@app.put(\"/api/patch/sync\")\n",
    "def patch_sync(input: InputPatchState):\n",
    "    \"\"\"\n",
    "    Sync hook patches between frontend and backend code\n",
    "\n",
    "    For the purpose of Transpector a patch is a hook that takes an activation from a slice of a\n",
    "    model component and applies it to a slice of another model component.\n",
    "\n",
    "    These two components are called the source and the target, source is where the copy happens\n",
    "    and target is where the paste is applied. To do this we use our cache that has saved the source\n",
    "    previously and add a hook on the target to apply from the saved cache.\n",
    "    \"\"\"\n",
    "\n",
    "    if input.clientLogicalClock >= ts.logical_clock:\n",
    "        ts.logical_clock += 1\n",
    "\n",
    "        temp_hooks_list: list[Hook] = []\n",
    "\n",
    "        for source_component_name, source_slices in input.patches.items():\n",
    "            print('transformer comp', source_component_name)\n",
    "            print('transformer comp', source_slices)\n",
    "\n",
    "            for _source_slice_id, source_slice_info in source_slices.items():\n",
    "                print('hook config', source_slice_info)\n",
    "\n",
    "                for target_component_name, target_slices in source_slice_info['edges'].items():\n",
    "                    for _target_slice_id, target_slice_config in target_slices.items():\n",
    "\n",
    "                        # When creating a patch we want to read from the source and transfer the\n",
    "                        # activations to the target, so the hook has to be triggered on the\n",
    "                        # target's name not the sources\n",
    "                        patch_hook = partial(\n",
    "                            ts.patch_hook,\n",
    "                            source_component=source_component_name,\n",
    "                            source_slice=source_slice_info['slice'],\n",
    "                            target_component=target_component_name,\n",
    "                            target_slice=target_slice_config['slice']\n",
    "                        )\n",
    "                        temp_hooks_list.append((target_component_name, patch_hook))\n",
    "\n",
    "        ts.fwd_patch_hooks = [hook for hook in temp_hooks_list]\n",
    "\n",
    "        ts.patches = input.patches\n",
    "\n",
    "    return {\n",
    "        \"server_logical_clock\": ts.logical_clock,\n",
    "        \"patches\": ts.patches\n",
    "    }\n",
    "\n",
    "app.mount(\"/\", StaticFiles(directory=\"../frontend_dist\", html=True), name=\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44a3595-22c7-4b0a-83c7-e2054f21fe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import uvicorn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app, port=8000)\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_running_loop()\n",
    "    loop.create_task(server.serve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
