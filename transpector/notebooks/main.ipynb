{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b988a2-6221-4b37-b2af-457ad71acea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e27cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/rkopel001/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
       "  from .autonotebook import tqdm as notebook_tqdm\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, Literal, Optional, Union, Sequence\n",
    "from fastapi import FastAPI\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from pydantic import BaseModel\n",
    "from torchtyping import TensorType as TT\n",
    "from model import get_available_models, get_pretrained_model_config, load_model, ActivationCache, HookPoint, Logits, per_token_losses, TensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de70f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NamesFilter = Optional[Union[Callable[[str], bool], Sequence[str]]]\n",
    "Hook = tuple[Union[str, Callable[..., Any]], Callable[..., Any]]\n",
    "\n",
    "\n",
    "SliceType = list[int] # [from, to]\n",
    "AblationTypes = Literal['zero', 'freeze']\n",
    "ModelComponent = str\n",
    "SliceName = str\n",
    "AblationSliceComponents = Literal['slice', 'ablationType']\n",
    "AblationsType = dict[ModelComponent, dict[SliceName, dict[AblationSliceComponents, list[SliceType] | AblationTypes ] ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf36f718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Using pad_token, but it is not set yet.\n",
       "Loaded pretrained model gpt2 into HookedTransformer\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Modelling:\n",
    "    def __init__(self):\n",
    "        self.logical_clock = 0\n",
    "\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.model = load_model(self.model_name)\n",
    "        self.model_config = get_pretrained_model_config(self.model_name)\n",
    "        self.available_models = get_available_models()\n",
    "        self.last_prompt: list[str] = []\n",
    "\n",
    "        self.ablations: AblationsType = {}\n",
    "        self.fwd_ablation_hooks: list[Hook] = []\n",
    "        self.bwd_ablation_hooks: list[Hook] = []\n",
    "\n",
    "    @property\n",
    "    def session_config(self):\n",
    "        return {\n",
    "            \"model_config\": self.model_config,\n",
    "            \"ablations\": self.ablations\n",
    "        }\n",
    "    \n",
    "    def set_model(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.model = load_model(self.model_name)\n",
    "        self.model_config = get_pretrained_model_config(self.model_name)\n",
    "\n",
    "    def run_with_hooks(\n",
    "            self,\n",
    "            prompt,\n",
    "            custom_fwd_hooks: Optional[list[Hook]]=None,\n",
    "            custom_bwd_hooks: Optional[list[Hook]]=None,\n",
    "            names_filter: NamesFilter = None,\n",
    "            device: Optional[str]=None,\n",
    "            remove_batch_dim: bool=False,\n",
    "            incl_bwd: bool=False,\n",
    "            reset_hooks_end: bool=True,\n",
    "            clear_contexts: bool=False,\n",
    "        ):\n",
    "        cache_dict, fwd, bwd = self.model.get_caching_hooks(\n",
    "            names_filter, incl_bwd, device, remove_batch_dim=remove_batch_dim\n",
    "        )\n",
    "\n",
    "        if custom_fwd_hooks:\n",
    "            fwd.extend(custom_fwd_hooks)\n",
    "\n",
    "        if custom_bwd_hooks:\n",
    "            bwd.extend(custom_bwd_hooks)\n",
    "\n",
    "        print('forward ablation hooks are')\n",
    "        print(self.fwd_ablation_hooks)\n",
    "        fwd = [*self.fwd_ablation_hooks, *fwd, *self.fwd_ablation_hooks]\n",
    "        bwd.extend(self.bwd_ablation_hooks)\n",
    "        print()\n",
    "        print(fwd)\n",
    "\n",
    "\n",
    "        with self.model.hooks(\n",
    "            fwd_hooks=fwd,\n",
    "            bwd_hooks=bwd,\n",
    "            reset_hooks_end=reset_hooks_end,\n",
    "            clear_contexts=clear_contexts,\n",
    "        ):\n",
    "            model_out_logits, model_out_loss = self.model(prompt, return_type='both')\n",
    "            if incl_bwd:\n",
    "                model_out.backward()\n",
    "\n",
    "        return model_out_logits, model_out_loss, cache_dict\n",
    "    \n",
    "    def ablation_hook(\n",
    "        self,\n",
    "        result: TensorType,\n",
    "        hook: HookPoint,\n",
    "        slices: list[SliceType],\n",
    "        ablation_type: AblationTypes='zero',\n",
    "    ) -> TensorType:\n",
    "        py_slice = [slice(r[0], r[1] if r[1]!=-1 else None) for r in slices]\n",
    "        \n",
    "        if ablation_type == 'zero':\n",
    "            result[py_slice] = 0.0\n",
    "\n",
    "        return result\n",
    "\n",
    "    def head_ablation_hook(\n",
    "        self,\n",
    "        attn_result: TT[\"batch\", \"seq\", \"n_heads\", \"d_model\"],\n",
    "        hook: HookPoint,\n",
    "    ) -> TT[\"batch\", \"seq\", \"n_heads\", \"d_model\"]:\n",
    "        \n",
    "        layer: int = hook.layer()\n",
    "        if layer in self.ablation_heads:\n",
    "            attn_result[:, :, self.ablation_heads[layer], :] = 0.0\n",
    "\n",
    "        return attn_result\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def clean_cache(cls, cache: ActivationCache):\n",
    "        \"\"\"\n",
    "        # Attrs to keep\n",
    "        'hook_embed',\n",
    "        'hook_pos_embed',\n",
    "        'blocks.*.hook_resid_pre',\n",
    "        'blocks.*.attn.hook_q',\n",
    "        'blocks.*.attn.hook_k',\n",
    "        'blocks.*.attn.hook_v',\n",
    "        'blocks.*.attn.hook_pattern',\n",
    "        'blocks.*.attn.hook_z',\n",
    "        'blocks.*.hook_attn_out',\n",
    "        'blocks.*.hook_resid_mid',\n",
    "        'blocks.*.hook_mlp_out',\n",
    "        'blocks.*.hook_resid_post',\n",
    "        'ln_final.hook_scale',\n",
    "        'ln_final.hook_normalized'\n",
    "\n",
    "        # Attrs to remove\n",
    "        'blocks.*.ln1.*'\n",
    "        'blocks.*.ln2.*'\n",
    "        'blocks.*.attn.hook_attn_scores',\n",
    "        'blocks.*.mlp.hook_pre',\n",
    "        'blocks.*.mlp.hook_post',\n",
    "        \"\"\"\n",
    "        cache_copy = {}\n",
    "        for key, value in cache.items():\n",
    "            match key.split('.'):\n",
    "                case ['hook_embed'] | ['hook_pos_embed'] | ['ln_final', 'hook_normalized']:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "                case ['blocks', blockno, ('hook_resid_pre' | 'hook_attn_out' | 'hook_resid_mid' | 'hook_mlp_out' | 'hook_resid_post')]:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "                case ['blocks', blockno, 'attn', ('hook_q' | 'hook_k' | 'hook_v' | 'hook_z' | 'hook_pattern')]:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "\n",
    "        return cache_copy\n",
    "    \n",
    "    def clean_logits(self, logits: Logits):\n",
    "        output_tokens: list[list[int]] = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "        if output_tokens.dim() == 1:\n",
    "            output_tokens = output_tokens.unsqueeze(0)\n",
    "\n",
    "        output_sub_words = [self.model.to_str_tokens(t) for t in output_tokens]\n",
    "        output_logits: list[list[list[float]]] = logits.tolist()\n",
    "\n",
    "        return  output_tokens.tolist(), output_sub_words, output_logits\n",
    "\n",
    "    @classmethod\n",
    "    def clean_loss(cls, loss: TensorType) -> float:\n",
    "        return loss.item()\n",
    "    \n",
    "    @classmethod\n",
    "    def clean_token_loss(cls, loss: TensorType) -> list[list[float]]:\n",
    "        return loss.tolist()\n",
    "\n",
    "\n",
    "ts = Modelling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543b3230-345a-4131-b82d-194057128f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"Hello\": \"World\"}\n",
    "\n",
    "@app.get(\"/api/models/getModels\")\n",
    "def get_models(): \n",
    "    return ts.available_models\n",
    "\n",
    "@app.get(\"/api/models/getModelConfig/{model_name}\")\n",
    "def get_model_config(model_name: str): \n",
    "    return {\n",
    "        \"config\": get_pretrained_model_config(model_name),\n",
    "        \"sessionConfig\": ts.session_config \n",
    "    }\n",
    "\n",
    "class ModelItem(BaseModel):\n",
    "    model_name: str\n",
    "\n",
    "@app.put(\"/api/models/setModel\")\n",
    "def set_models(model_name: ModelItem):\n",
    "    print(f'Loading model: {model_name.model_name}')\n",
    "    ts.set_model(model_name.model_name)\n",
    "    print('Loaded model')\n",
    "    return {\"Loaded model\": model_name}\n",
    "\n",
    "class InputStringListItem(BaseModel):\n",
    "    input: list[str]\n",
    "\n",
    "class InputIntListItem(BaseModel):\n",
    "    input: list[int]\n",
    "\n",
    "@app.put(\"/api/tokenize/toStringTokens\")\n",
    "def tokenize_to_string_tokens(inputItem: InputStringListItem):\n",
    "    return {\"stringTokens\": ts.model.to_str_tokens(inputItem.input)}\n",
    "\n",
    "@app.put(\"/api/tokenize/toTokens\")\n",
    "def tokenize_to_tokens(inputItem: InputStringListItem) -> dict[str, list[list[int]]]:\n",
    "    print('tokenizing', inputItem.input)\n",
    "    ts.last_prompt = inputItem.input\n",
    "    print('tokenizing', ts.model.to_tokens(inputItem.input).tolist())\n",
    "    return {\"tokens\": ts.model.to_tokens(inputItem.input).tolist()}\n",
    "\n",
    "@app.put(\"/api/tokenize/toString\")\n",
    "def tokenize_to_string(inputItem: InputIntListItem):\n",
    "    return {\"string\": ts.model.to_tokens(inputItem.input)}\n",
    "\n",
    "\n",
    "@app.get(\"/api/inference/run\")\n",
    "def inference_run():\n",
    "    prompt = ts.last_prompt\n",
    "    sub_words = ts.model.to_str_tokens(prompt)\n",
    "\n",
    "    logits, loss, cache = ts.run_with_hooks(prompt)\n",
    "    print('logtis are')\n",
    "    print(logits)\n",
    "    out_cache = ts.clean_cache(cache)\n",
    "    out_tokens, out_sub_words, out_logits = ts.clean_logits(logits)\n",
    "    out_final_loss = ts.clean_loss(loss)\n",
    "    out_token_loss = ts.clean_token_loss(per_token_losses(logits, ts.model.to_tokens(prompt)))\n",
    "    return {\n",
    "        \"activationData\": out_cache,\n",
    "        \"inferencePrompt\": prompt,\n",
    "        \"inferenceSubWords\": sub_words,\n",
    "        \"logits\": out_logits,\n",
    "        \"tokens\": out_tokens,\n",
    "        \"subWords\": out_sub_words,\n",
    "        \"finalLoss\": out_final_loss,\n",
    "        \"tokenLoss\": out_token_loss,\n",
    "    }\n",
    "    \n",
    "class InputAblationState(BaseModel):\n",
    "    ablations: AblationsType\n",
    "    clientLogicalClock: int\n",
    "\n",
    "\n",
    "@app.put(\"/api/ablation/sync\")\n",
    "def ablation_sync(input: InputAblationState):\n",
    "    print('input', input)\n",
    "    if input.clientLogicalClock >= ts.logical_clock:\n",
    "        print('Im here')\n",
    "        ts.logical_clock += 1\n",
    "\n",
    "        ts.fwd_ablation_hooks = []\n",
    "\n",
    "        for transformer_component_name, component_slices in input.ablations.items():\n",
    "            print('transformer comp', transformer_component_name)\n",
    "            print('transformer comp', component_slices)\n",
    "            for _, hook_config in component_slices.items():\n",
    "                print('hook config', hook_config)\n",
    "                ablation_hook = partial(ts.ablation_hook, slices=hook_config['slice'], ablation_type=hook_config['ablationType'])\n",
    "                ts.fwd_ablation_hooks.append((transformer_component_name, ablation_hook))\n",
    "\n",
    "        ts.ablations = input.ablations\n",
    "\n",
    "    return {\n",
    "        \"server_logical_clock\": ts.logical_clock,\n",
    "        \"ablations\": ts.ablations\n",
    "    }\n",
    "\n",
    "app.mount(\"/\", StaticFiles(directory=\"../frontend_dist\", html=True), name=\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44a3595-22c7-4b0a-83c7-e2054f21fe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import uvicorn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app, port=8000)\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_running_loop()\n",
    "    loop.create_task(server.serve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
