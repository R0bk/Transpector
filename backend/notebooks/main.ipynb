{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b988a2-6221-4b37-b2af-457ad71acea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkopel001/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543b3230-345a-4131-b82d-194057128f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from model import get_available_models, get_pretrained_model_config, load_model, ActivationCache\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Modelling:\n",
    "    model = load_model('gpt2')\n",
    "    available_models = get_available_models()\n",
    "    last_prompt: list[str] = []\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def clean_cache(cls, cache: ActivationCache):\n",
    "        \"\"\"\n",
    "        # Attention vals to keep\n",
    "        'hook_embed',\n",
    "        'hook_pos_embed',\n",
    "        'blocks.*.hook_resid_pre',\n",
    "        'blocks.*.attn.hook_q',\n",
    "        'blocks.*.attn.hook_k',\n",
    "        'blocks.*.attn.hook_v',\n",
    "        'blocks.*.attn.hook_pattern',\n",
    "        'blocks.*.attn.hook_z',\n",
    "        'blocks.*.hook_attn_out',\n",
    "        'blocks.*.hook_resid_mid',\n",
    "        'blocks.*.hook_mlp_out',\n",
    "        'blocks.*.hook_resid_post',\n",
    "        'ln_final.hook_scale',\n",
    "        'ln_final.hook_normalized'\n",
    "\n",
    "        # Attention vals to remove\n",
    "        'blocks.*.ln1.*'\n",
    "        'blocks.*.ln2.*'\n",
    "        'blocks.*.attn.hook_attn_scores',\n",
    "        'blocks.*.mlp.hook_pre',\n",
    "        'blocks.*.mlp.hook_post',\n",
    "        \"\"\"\n",
    "        cache_copy = {}\n",
    "        for key, value in cache.items():\n",
    "            match key.split('.'):\n",
    "                case ['hook_embed'] | ['hook_pos_embed'] | ['ln_final', 'hook_normalized']:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "                case ['blocks', blockno, ('hook_resid_pre' | 'hook_attn_out' | 'hook_resid_mid' | 'hook_mlp_out' | 'hook_resid_post')]:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "                case ['blocks', blockno, 'attn', ('hook_q' | 'hook_k' | 'hook_v' | 'hook_z' | 'hook_pattern')]:\n",
    "                    cache_copy[key] = value.tolist()\n",
    "\n",
    "        return cache_copy\n",
    "\n",
    "@app.get(\"/api/models/getModels\")\n",
    "def get_models(): \n",
    "    return Modelling.available_models\n",
    "\n",
    "@app.get(\"/api/models/getModelConfig/{model_name}\")\n",
    "def get_model_config(model_name: str): \n",
    "    return {\"config\": get_pretrained_model_config(model_name)}\n",
    "\n",
    "class ModelItem(BaseModel):\n",
    "    model_name: str\n",
    "\n",
    "@app.put(\"/api/models/setModel\")\n",
    "def set_models(model_name: ModelItem):\n",
    "    print(f'Loading model: {model_name.model_name}')\n",
    "    Modelling.model = load_model(model_name.model_name)\n",
    "    print('Loaded model')\n",
    "    return {\"Loaded model\": model_name}\n",
    "\n",
    "class InputStringListItem(BaseModel):\n",
    "    input: list[str]\n",
    "\n",
    "class InputIntListItem(BaseModel):\n",
    "    input: list[int]\n",
    "\n",
    "@app.put(\"/api/tokenize/toStringTokens\")\n",
    "def tokenize_to_string_tokens(inputItem: InputStringListItem):\n",
    "    return {\"stringTokens\": Modelling.model.to_str_tokens(inputItem.input)}\n",
    "\n",
    "@app.put(\"/api/tokenize/toTokens\")\n",
    "def tokenize_to_tokens(inputItem: InputStringListItem) -> dict[str, list[list[int]]]:\n",
    "    print('tokenizing', inputItem.input)\n",
    "    Modelling.last_prompt = inputItem.input\n",
    "    print('tokenizing', Modelling.model.to_tokens(inputItem.input).tolist())\n",
    "    return {\"tokens\": Modelling.model.to_tokens(inputItem.input).tolist()}\n",
    "\n",
    "@app.put(\"/api/tokenize/toString\")\n",
    "def tokenize_to_string(inputItem: InputIntListItem):\n",
    "    return {\"string\": Modelling.model.to_tokens(inputItem.input)}\n",
    "\n",
    "@app.get(\"/api/inference/run\")\n",
    "def inference_run():\n",
    "    prompt = Modelling.last_prompt\n",
    "    logits, cache = Modelling.model.run_with_cache(prompt)\n",
    "    cleaned_cache = Modelling.clean_cache(cache)\n",
    "    return {\"activationData\": cleaned_cache, \"inferencePrompt\": prompt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a3595-22c7-4b0a-83c7-e2054f21fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import uvicorn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app, port=9000)\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_running_loop()\n",
    "    loop.create_task(server.serve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
